{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIWintermuteAI/Speech-to-Intent-Micro/blob/main/jupyter_notebooks/speech_to_intent_tf_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08eb977b",
      "metadata": {
        "id": "08eb977b"
      },
      "source": [
        "# Speech-to-Intent Model Training and Conversion to Tensorflow Lite for Microcontrollers\n",
        "\n",
        "Follow this notebook to understand the workflow and the internal workings of the Speech-to-Intent application. The training script includes more options and other model architectures.\n",
        "\n",
        "First we do the necessary imports. Before running this notebook make sure you installed the necessary packages with\n",
        "```pip install -r requiremnts.txt```\n",
        "It is preferred to either use virtualenv or conda.\n",
        "\n",
        "Change the path to dataset .csv files and parameters of audio processing in the next cell if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EesM_okCJ6Mn",
      "metadata": {
        "id": "EesM_okCJ6Mn"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !gdown https://drive.google.com/uc?id=1di8A3zPfrbByBgUfhP1kJnzClKj0gZo5  #dataset\n",
        "    !unzip --qq slu_data.zip\n",
        "    !pip install audiomentations\n",
        "    !sudo apt-get install xxd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GM3FdoxY8i8C",
      "metadata": {
        "id": "GM3FdoxY8i8C"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d50f4bba",
      "metadata": {
        "id": "d50f4bba"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "import io, base64\n",
        "import os, sys\n",
        "from datetime import datetime\n",
        "\n",
        "import IPython\n",
        "import pickle\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import string\n",
        "\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "import soundfile as sf\n",
        "from audiomentations import Compose, AddGaussianNoise, AddBackgroundNoise, PitchShift, Shift, ClippingDistortion, Gain, LoudnessNormalization, TimeStretch\n",
        "from tensorflow.python.ops import gen_audio_ops as contrib_audio\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Reshape, Flatten, Activation, Add\n",
        "from tensorflow.keras.layers import Dense, Dropout, Softmax, TimeDistributed, LSTM\n",
        "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D\n",
        "from tensorflow.keras.activations import relu\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, ZeroPadding2D\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "import random\n",
        "import glob\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import scipy.io.wavfile as wav\n",
        "\n",
        "DEBUG = False\n",
        "\n",
        "root_folder = '.' if IN_COLAB else \"..\"\n",
        "project_path = os.path.join(root_folder, 'checkpoints')\n",
        "\n",
        "train_dataset_path = os.path.join(root_folder, 'data/csv/train_data.csv')\n",
        "valid_dataset_path = os.path.join(root_folder, 'data/csv/valid_data.csv')\n",
        "#test_dataset_path = os.path.join(root_folder, 'data/csv/test_data.csv')\n",
        "test_dataset_path = os.path.join(root_folder, 'data/csv/wt_data.csv')\n",
        "\n",
        "SAMPLING_RATE = 16000\n",
        "MIN_FREQ = 100\n",
        "MAX_FREQ = SAMPLING_RATE//2\n",
        "WIN_SIZE_MS = 0.02\n",
        "WIN_INCREASE_MS = 0.02\n",
        "NUM_CEPSTRAL = 10\n",
        "EPOCH_NUM = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da46a479",
      "metadata": {
        "id": "da46a479"
      },
      "source": [
        "Let's open the test dataset and have a look at it's content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8534cb9e",
      "metadata": {
        "id": "8534cb9e"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(test_dataset_path)\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b31ca779",
      "metadata": {
        "id": "b31ca779"
      },
      "source": [
        "For a quick dataset sanity check - especially useful if you are crearing the dataset yourself, let's listen to the first entry and make sure it matches the transcription."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb50f4e",
      "metadata": {
        "id": "7eb50f4e"
      },
      "outputs": [],
      "source": [
        "prefix = [root_folder, \"data\"]\n",
        "wav_file = os.path.join(*prefix, test_data['path'][0])\n",
        "print(wav_file)\n",
        "IPython.display.Audio(wav_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9445518",
      "metadata": {
        "id": "e9445518"
      },
      "source": [
        "In order to match MFCC processing parameters on device, we use audio_spectrogram and mfcc functions from Tensorflow gen_audio_ops. In generate_features function we create a spectrogram, convert it to mel frequency and (optionally) visualize it with matplotlib. In the next cell we run this process for the above .wav file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9ef34df",
      "metadata": {
        "id": "a9ef34df"
      },
      "outputs": [],
      "source": [
        "audio, sample_rate = librosa.load(wav_file, sr=16000, res_type='kaiser_best')\n",
        "\n",
        "if DEBUG:\n",
        "    print(wav_file)\n",
        "\n",
        "def generate_features(draw_graphs, raw_data, sampling_freq,\n",
        "                      frame_length, frame_stride, num_filters,\n",
        "                      num_cepstral, low_frequency, high_frequency):\n",
        "    graphs = []\n",
        "\n",
        "    raw_data = np.expand_dims(raw_data, axis = -1)\n",
        "    window_size = int(sampling_freq * frame_length)\n",
        "    stride = int(sampling_freq * frame_stride)\n",
        "\n",
        "    spectrogram = contrib_audio.audio_spectrogram(\n",
        "        raw_data,\n",
        "        window_size=window_size,\n",
        "        stride=stride,\n",
        "        magnitude_squared=True)\n",
        "\n",
        "    mfcc = contrib_audio.mfcc(\n",
        "        spectrogram,\n",
        "        sampling_freq,\n",
        "        dct_coefficient_count=num_cepstral,\n",
        "        upper_frequency_limit=high_frequency,\n",
        "        lower_frequency_limit=low_frequency)\n",
        "\n",
        "    mfcc = np.squeeze(mfcc)\n",
        "\n",
        "    if draw_graphs:\n",
        "        mfcc_graph = np.swapaxes(mfcc, 0, 1)\n",
        "        fig, ax = plt.subplots()\n",
        "        img = librosa.display.specshow(mfcc_graph, x_axis='time', ax=ax)\n",
        "        fig.colorbar(img, ax=ax)\n",
        "        ax.set(title='MFCC')\n",
        "        buf = io.BytesIO()\n",
        "\n",
        "        plt.savefig(buf, format='svg', bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "        buf.seek(0)\n",
        "        image = (base64.b64encode(buf.getvalue()).decode('ascii'))\n",
        "\n",
        "        buf.close()\n",
        "\n",
        "        graphs.append({\n",
        "            'name': 'Cepstral Coefficients',\n",
        "            'image': image,\n",
        "            'imageMimeType': 'image/svg+xml',\n",
        "            'type': 'image'\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'features': mfcc,\n",
        "        'graphs': graphs,\n",
        "        'output_config': {\n",
        "            'type': 'spectrogram',\n",
        "            'shape': {\n",
        "                'width': mfcc.shape[1],\n",
        "                'height': mfcc.shape[0]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "processed = generate_features(True, audio, SAMPLING_RATE,\n",
        "                              WIN_SIZE_MS, WIN_INCREASE_MS, 32,\n",
        "                              NUM_CEPSTRAL, MIN_FREQ, MAX_FREQ)\n",
        "\n",
        "if DEBUG:\n",
        "    print(processed['features'])\n",
        "\n",
        "print(processed['output_config'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae020d7",
      "metadata": {
        "id": "5ae020d7"
      },
      "source": [
        "In the next cell we process the .csv file data into ground truth labels for the model. Slots include both words for objects and locations, while intents include only words for actions. During preprocessing we also create a common vocabulary, including all the words in transcriptions, slots and intents, but it is not used in training.\n",
        "In order to ensure the consistency between runs, as long as you use the same dataset, it only necessary to create and save vectorized training/validation data and reverse dictionaries (for decoding) once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb8c0f5",
      "metadata": {
        "id": "cbb8c0f5"
      },
      "outputs": [],
      "source": [
        "class DatasetFactory:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.actions = set()\n",
        "        self.objects = set()\n",
        "        self.locations = set()\n",
        "        self.vocab = set()\n",
        "\n",
        "    def get_query_slots(self, sentence):\n",
        "\n",
        "        slots = [sentence[0], sentence[1]]\n",
        "        return slots\n",
        "\n",
        "    def get_properties(self, data):\n",
        "\n",
        "        data[\"action\"] = data['action'].str.lower()\n",
        "        data[\"object\"] = data['object'].str.lower()\n",
        "        data[\"location\"] = data['location'].str.lower()\n",
        "\n",
        "        actions = set(data.action.unique())\n",
        "        objects = set(data.object.unique())\n",
        "        locations = set(data.location.unique())\n",
        "\n",
        "        return actions, objects, locations\n",
        "\n",
        "    def get_vocab(self, actions, objects, locations, data):\n",
        "\n",
        "        vocab = objects | locations\n",
        "\n",
        "        if DEBUG:\n",
        "            print(vocab)\n",
        "\n",
        "        data[\"transcription\"] = data['transcription'].str.replace('[^\\w\\s]','')\n",
        "        data[\"transcription\"] = data['transcription'].str.lower()\n",
        "\n",
        "        for item in data.transcription:\n",
        "            for word in item.split(\" \"):\n",
        "                vocab.add(word)\n",
        "\n",
        "        vocab = [s.strip() for s in vocab]\n",
        "\n",
        "        return set(vocab)\n",
        "\n",
        "    def add_corpora(self, data):\n",
        "\n",
        "        actions, objects, locations = self.get_properties(data)\n",
        "        vocab = self.get_vocab(actions, objects, locations, data)\n",
        "\n",
        "        self.actions = set(self.actions | actions)\n",
        "        self.objects = set(self.objects | objects)\n",
        "        self.locations = set(self.locations | locations)\n",
        "        self.vocab = set(self.vocab | vocab)\n",
        "        self.query_slots = set(self.objects | self.locations)\n",
        "\n",
        "    def process_data(self, data):\n",
        "\n",
        "        self.actions = list(self.actions)\n",
        "        self.objects = list(self.objects)\n",
        "        self.locations = list(self.locations)\n",
        "        self.vocab = list(self.vocab)\n",
        "        self.query_slots = list(self.query_slots)\n",
        "\n",
        "        word_ids, slot_ids, intent_ids = {' ': 0}, {}, {self.actions[i]: i for i in range(0, len(self.actions))}\n",
        "\n",
        "        slots = []\n",
        "        for sentence in zip(data.object, data.location):\n",
        "            slots.append(self.get_query_slots(sentence))\n",
        "\n",
        "        i = 0\n",
        "        for slot in self.query_slots:\n",
        "            if slot == 'none':\n",
        "                continue\n",
        "            slot_ids[slot] = i\n",
        "            i += 1\n",
        "\n",
        "        slot_ids['none'] = i\n",
        "\n",
        "        #convert vocab to dictionary\n",
        "        start = 1\n",
        "        for i in range(len(self.vocab)):\n",
        "            word_ids[self.vocab[i]] = start + i\n",
        "        word_ids['unknown'] =  i + 1\n",
        "\n",
        "        #create reverse dicts\n",
        "        ids2words = dict((v, k) for k, v in word_ids.items())\n",
        "        ids2slots = dict((v, k) for k, v in slot_ids.items())\n",
        "        ids2intents = dict((v, k) for k, v in intent_ids.items())\n",
        "\n",
        "        n_vocab = len(ids2words)\n",
        "\n",
        "        n_classes = len(ids2intents)\n",
        "        n_slots = len(ids2slots)\n",
        "\n",
        "        vectorized_slots = list(map(lambda slots: np.array(list(map(lambda slot: slot_ids[slot], slots))), slots))\n",
        "        vectorized_intents = list(map(lambda l: np.array([intent_ids[l]]), data.action))\n",
        "\n",
        "        filepaths = data['path'].to_numpy()\n",
        "\n",
        "        return ids2intents, ids2slots, vectorized_slots, vectorized_intents, filepaths\n",
        "\n",
        "def save_obj(obj, name):\n",
        "    with open(os.path.join(root_folder, 'data/pkl/'+ name + '.pkl'), 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(os.path.join(root_folder, 'data/pkl/'+ name + '.pkl'), 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "generate_data = True #change that to True the first time you are running the code\n",
        "\n",
        "train_data = pd.read_csv(train_dataset_path)\n",
        "valid_data = pd.read_csv(valid_dataset_path)\n",
        "test_data = pd.read_csv(test_dataset_path)\n",
        "\n",
        "if generate_data:\n",
        "\n",
        "    dataset_processor = DatasetFactory()\n",
        "\n",
        "    train_data = pd.read_csv(train_dataset_path)\n",
        "    valid_data = pd.read_csv(valid_dataset_path)\n",
        "    test_data = pd.read_csv(test_dataset_path)\n",
        "\n",
        "    dataset_processor.add_corpora(train_data)\n",
        "    dataset_processor.add_corpora(valid_data)\n",
        "    dataset_processor.add_corpora(test_data)\n",
        "\n",
        "    ids2intents, ids2slots, vectorized_slots_train, vectorized_intents_train, filepaths_train = dataset_processor.process_data(train_data)\n",
        "    _ids2intents, _ids2slots, vectorized_slots_valid, vectorized_intents_valid, filepaths_valid = dataset_processor.process_data(valid_data)\n",
        "    __ids2intents, __ids2slots, vectorized_slots_test, vectorized_intents_test, filepaths_test = dataset_processor.process_data(test_data)\n",
        "\n",
        "    assert ids2intents == _ids2intents == __ids2intents\n",
        "    assert ids2slots == _ids2slots == __ids2slots\n",
        "\n",
        "    save_obj(ids2intents, 'ids2intents')\n",
        "    save_obj(ids2slots, 'ids2slots')\n",
        "\n",
        "    save_obj(vectorized_slots_train, 'vectorized_slots_train')\n",
        "    save_obj(vectorized_intents_train, 'vectorized_intents_train')\n",
        "\n",
        "    save_obj(vectorized_slots_valid, 'vectorized_slots_valid')\n",
        "    save_obj(vectorized_intents_valid, 'vectorized_intents_valid')\n",
        "\n",
        "    save_obj(vectorized_slots_test, 'vectorized_slots_test')\n",
        "    save_obj(vectorized_intents_test, 'vectorized_intents_test')\n",
        "\n",
        "else:\n",
        "\n",
        "    filepaths_train = train_data['path'].to_numpy()\n",
        "    filepaths_valid = valid_data['path'].to_numpy()\n",
        "    filepaths_test = test_data['path'].to_numpy()\n",
        "\n",
        "    ids2intents = load_obj('ids2intents')\n",
        "    ids2slots = load_obj('ids2slots')\n",
        "\n",
        "    vectorized_slots_train = load_obj('vectorized_slots_train')\n",
        "    vectorized_intents_train = load_obj('vectorized_intents_train')\n",
        "\n",
        "    vectorized_slots_valid = load_obj('vectorized_slots_valid')\n",
        "    vectorized_intents_valid = load_obj('vectorized_intents_valid')\n",
        "\n",
        "    vectorized_slots_test = load_obj('vectorized_slots_test')\n",
        "    vectorized_intents_test = load_obj('vectorized_intents_test')\n",
        "\n",
        "if DEBUG:\n",
        "    print(vectorized_slots_test)\n",
        "    print(vectorized_intents_test)\n",
        "    print(ids2intents)\n",
        "    print(ids2slots)\n",
        "\n",
        "print(str(ids2intents.values()).replace(\"'\", \"\\\"\"))\n",
        "print(str(ids2slots.values()).replace(\"'\", \"\\\"\"))\n",
        "\n",
        "n_classes = len(ids2intents)\n",
        "n_slots = len(ids2slots)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74f321c6",
      "metadata": {
        "id": "74f321c6"
      },
      "source": [
        "Below data generator class is create and instantiated. In the actual training we limiut the length of samples to 3 seconds by zero padding shorter samples and trimming the longer. Additionally since training data only contains noise-free samples we apply aggressive audio augmentation pipeline. While applying data agumentation is slower than augmenting MFCC features directly, it is much easier to examine augmented augio samples than it is inspect MFCC visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc5f30d",
      "metadata": {
        "id": "8cc5f30d"
      },
      "outputs": [],
      "source": [
        "def create_aug_pipeline():\n",
        "\n",
        "    aug_pipeline = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.1),\n",
        "    AddBackgroundNoise(sounds_path=os.path.join(root_folder, \"data/wavs/background_noise\"), p=0.3),\n",
        "    ClippingDistortion(p=0.3),\n",
        "    PitchShift(min_semitones=-4, max_semitones=4, p=0.2),\n",
        "    Shift(min_shift=-0.5, max_shift=0.5, p=0.1),\n",
        "    Gain(p=0.2),\n",
        "    TimeStretch(p=0.05)\n",
        "    ])\n",
        "\n",
        "    return aug_pipeline\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    \"\"\"Generates data for Keras\n",
        "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
        "    \"\"\"\n",
        "    def __init__(self, entries, num_list, batch_size, shuffle=True, to_fit=True, augment = True, vis = False):\n",
        "\n",
        "        self.entries = entries\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.n_intents, self.n_slots = num_list\n",
        "\n",
        "        self.len = 2\n",
        "        self.aug_pipeline = None\n",
        "        if augment:\n",
        "            self.aug_pipeline = create_aug_pipeline()\n",
        "        self.vis = vis\n",
        "        self.shuffle = shuffle\n",
        "        self.to_fit = to_fit\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\n",
        "        :return: number of batches per epoch\n",
        "        \"\"\"\n",
        "        return int(np.floor(len(self.entries[0]) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\n",
        "        :param index: index of the batch\n",
        "        :return: X and y when fitting. X only when predicting\n",
        "        \"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        X_batch = [self.entries[0][k] for k in indexes]\n",
        "\n",
        "        Y_intent = [self.entries[1][k] for k in indexes]\n",
        "        Y_slot = [self.entries[2][k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X = self._generate_X(X_batch)\n",
        "\n",
        "        if self.to_fit:\n",
        "            y = self._generate_y(Y_intent, Y_slot)\n",
        "            return X, y\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\n",
        "        \"\"\"\n",
        "        self.indexes = np.arange(len(self.entries[0]))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def _generate_X(self, batch_items):\n",
        "\n",
        "        X = np.zeros(shape = (self.batch_size, 150, NUM_CEPSTRAL, 1))\n",
        "\n",
        "        for i, batch_item in enumerate(batch_items):\n",
        "            wav_file = os.path.join(*prefix, batch_item)\n",
        "            audio, sample_rate = librosa.load(os.path.join(wav_file), sr=16000, res_type='kaiser_best')\n",
        "            audio = librosa.util.fix_length(data = audio, size = 16000*3)\n",
        "\n",
        "            if self.aug_pipeline:\n",
        "                audio = self.aug_pipeline(audio, sample_rate)\n",
        "\n",
        "                if DEBUG:\n",
        "                    new_filename = os.path.join('samples', os.path.basename(batch_item.split('.')[0]+'aug.wav'))\n",
        "                    print(\"Augmented: \", new_filename)\n",
        "                    print(\"--------------\")\n",
        "                    sf.write(new_filename, audio, sample_rate,  subtype='PCM_16')\n",
        "\n",
        "            output = generate_features(self.vis, audio, SAMPLING_RATE,\n",
        "                                          WIN_SIZE_MS, WIN_INCREASE_MS, 32,\n",
        "                                          NUM_CEPSTRAL, MIN_FREQ, MAX_FREQ)\n",
        "\n",
        "            features = output['features']\n",
        "            X[i, ] = np.expand_dims(features, axis = -1)\n",
        "        return X\n",
        "\n",
        "    def _generate_y(self, intents, slots):\n",
        "        intent_y = np.empty((self.batch_size, self.n_intents), dtype=int)\n",
        "        slot_y = np.empty((self.batch_size, self.len, self.n_slots), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, batch_item in enumerate(intents):\n",
        "            intent = intents[i]\n",
        "            slot = slots[i]\n",
        "            intent_y[i,] = np.eye(self.n_intents)[intent]\n",
        "            slot_y[i,] = np.eye(self.n_slots)[slot][np.newaxis, :]\n",
        "\n",
        "        return [intent_y, slot_y]\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "training_generator = DataGenerator([filepaths_train, vectorized_intents_train, vectorized_slots_train],\n",
        "                                   [n_classes, n_slots], batch_size = batch_size,\n",
        "                                   shuffle=True, to_fit=True, augment = True)\n",
        "\n",
        "data = training_generator.__getitem__(0)\n",
        "print(data[0].shape)\n",
        "print(data[1][0].shape)\n",
        "print(data[1][1].shape)\n",
        "print(training_generator.__len__())\n",
        "\n",
        "validation_generator = DataGenerator([filepaths_valid, vectorized_intents_valid, vectorized_slots_valid],\n",
        "                                     [n_classes, n_slots], batch_size = batch_size,\n",
        "                                     shuffle=False, to_fit=True, augment = False)\n",
        "\n",
        "\n",
        "data = validation_generator.__getitem__(0)\n",
        "print(data[0].shape)\n",
        "print(data[1][0].shape)\n",
        "print(data[1][1].shape)\n",
        "print(validation_generator.__len__())\n",
        "\n",
        "test_generator = DataGenerator([filepaths_test, vectorized_intents_test, vectorized_slots_test],\n",
        "                                     [n_classes, n_slots], batch_size = batch_size,\n",
        "                                     shuffle=False, to_fit=True, augment = False)\n",
        "\n",
        "data = test_generator.__getitem__(0)\n",
        "print(data[0].shape)\n",
        "print(data[1][0].shape)\n",
        "print(data[1][1].shape)\n",
        "print(test_generator.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d3e3c11",
      "metadata": {
        "id": "9d3e3c11"
      },
      "source": [
        "#### Basic feature extractor (Vanilla Conv2D)\n",
        "This model architecture is very basic and consists of 2D Convolutional layers followed by Batch Normalization and (except for first and last layer) Max Pooling 2D layers. Then Global Max Pooling is applied and resulting feature maps are fed into Dense layer and then to slot and intent outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab33e5a9",
      "metadata": {
        "id": "ab33e5a9"
      },
      "outputs": [],
      "source": [
        "K.clear_session()\n",
        "\n",
        "main_input = Input(shape=(150, NUM_CEPSTRAL, 1), name='main_input')\n",
        "\n",
        "x = Conv2D(16, 3, padding='same', activation='relu', use_bias = False)(main_input)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Conv2D(16, 2, padding='same', activation='relu', use_bias = False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size = (2,2))(x)\n",
        "\n",
        "x = Conv2D(16, 2, padding='same', activation='relu', use_bias = False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size = (2,2))(x)\n",
        "\n",
        "x = Conv2D(32, 3, padding='same', activation='relu', use_bias = False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size = (2,2))(x)\n",
        "\n",
        "x = Conv2D(128, 2, padding='same', activation='relu', use_bias = False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "slot_dense = Dense(n_slots*2)(x)\n",
        "slot_reshape = Reshape(target_shape = (2, n_slots))(slot_dense)\n",
        "slot_output = Softmax(name='slot_output')(slot_reshape)\n",
        "\n",
        "intent_output = Dense(n_classes, activation='softmax', name='intent_output', use_bias = False)(x)\n",
        "\n",
        "model = Model(inputs=main_input, outputs=[intent_output, slot_output])\n",
        "\n",
        "optim = Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(optimizer = optim, loss='categorical_crossentropy', metrics='accuracy')\n",
        "model.summary()\n",
        "\n",
        "if DEBUG:\n",
        "  tf.keras.utils.plot_model(model, to_file='img.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c7ccfe",
      "metadata": {
        "id": "f8c7ccfe"
      },
      "outputs": [],
      "source": [
        "output_path = os.path.join(project_path, datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "os.makedirs(output_path)\n",
        "print(\"Project folder: {}\".format(output_path))\n",
        "\n",
        "model_name = os.path.join(output_path, \"slu_model.h5\")\n",
        "log_dir =  os.path.join(output_path, \"logs\")\n",
        "\n",
        "my_callbacks = [\n",
        "    EarlyStopping(patience=10, restore_best_weights=True, verbose = 1),\n",
        "    ModelCheckpoint(filepath=model_name, save_best_only=True, verbose = 1),\n",
        "    TensorBoard(log_dir=log_dir),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose = 1)]\n",
        "try:\n",
        "    model.fit(training_generator, validation_data = test_generator,\n",
        "              callbacks = my_callbacks, epochs = EPOCH_NUM,\n",
        "              workers = 4, max_queue_size = 10,\n",
        "              use_multiprocessing = False)\n",
        "except KeyboardInterrupt:\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a235cd9",
      "metadata": {
        "id": "3a235cd9"
      },
      "source": [
        "### LSTM layer instead of Fully-Connected + Time Distributed\n",
        "\n",
        "One of the biggest disadvantages of the baseline model is that it fails to preserve temporal dependencies in the output of feature extractor. In order to preserve the temporal component of the signal, we can utilize a single LSTM layer after Conv2D wrapped with TimeDistributed layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d23bdd",
      "metadata": {
        "id": "a5d23bdd"
      },
      "outputs": [],
      "source": [
        "K.clear_session()\n",
        "\n",
        "main_input = Input(shape=(150, NUM_CEPSTRAL, 1), name='main_input')\n",
        "\n",
        "x = Conv2D(16, 3, padding='same', activation='relu', use_bias = False)(main_input)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Conv2D(16, 2, padding='same', activation='relu', use_bias = False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size = (2,2))(x)\n",
        "\n",
        "x = Conv2D(16, 2, padding='same', activation='relu', use_bias = False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size = (2,2))(x)\n",
        "\n",
        "x = Conv2D(32, 3, padding='same', activation='relu', use_bias = False)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size = (2,2))(x)\n",
        "\n",
        "x = Conv2D(128, 2, padding='same', activation='relu', use_bias = False)(x)\n",
        "x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "x = LSTM(32, activation='relu')(x)\n",
        "\n",
        "slot_dense = Dense(n_slots*2)(x)\n",
        "slot_reshape = Reshape(target_shape = (2, n_slots))(slot_dense)\n",
        "slot_output = Softmax(name='slot_output')(slot_reshape)\n",
        "\n",
        "intent_output = Dense(n_classes, activation='softmax', name='intent_output', use_bias = False)(x)\n",
        "\n",
        "model = Model(inputs=main_input, outputs=[intent_output, slot_output])\n",
        "\n",
        "optim = Adam(learning_rate=1e-3, weight_decay=1e-6)\n",
        "\n",
        "model.compile(optimizer = optim, loss='categorical_crossentropy', metrics='accuracy')\n",
        "model.summary()\n",
        "\n",
        "if DEBUG:\n",
        "  tf.keras.utils.plot_model(model, to_file='img.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkvU9E-CIh_u"
      },
      "outputs": [],
      "source": [
        "output_path = os.path.join(project_path, datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "os.makedirs(output_path)\n",
        "print(\"Project folder: {}\".format(output_path))\n",
        "\n",
        "model_name = os.path.join(output_path, \"slu_model.h5\")\n",
        "log_dir =  os.path.join(output_path, \"logs\")\n",
        "\n",
        "my_callbacks = [\n",
        "    EarlyStopping(patience=10, restore_best_weights=True, verbose = 1),\n",
        "    ModelCheckpoint(filepath=model_name, save_best_only=True, verbose = 1),\n",
        "    TensorBoard(log_dir=log_dir),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose = 1)]\n",
        "try:\n",
        "    model.fit(training_generator, validation_data = test_generator,\n",
        "              callbacks = my_callbacks, epochs = EPOCH_NUM,\n",
        "              workers = 4, max_queue_size = 10,\n",
        "              use_multiprocessing = False)\n",
        "except KeyboardInterrupt:\n",
        "    raise"
      ],
      "id": "zkvU9E-CIh_u"
    },
    {
      "cell_type": "markdown",
      "id": "238ef3a1",
      "metadata": {
        "id": "238ef3a1"
      },
      "source": [
        "### Residual connections\n",
        "\n",
        "Another attempt at improving the quality of the network predictions is adding skip connections by utilizing ResNet blocks. The hope is that some of the low level feature information will be passed on to top levels of feature extractor and used when making predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424a25b2",
      "metadata": {
        "id": "424a25b2"
      },
      "outputs": [],
      "source": [
        "def make_residual_block(X, num_channels, use_1x1conv=False, strides=1):\n",
        "        conv1 = Conv2D(num_channels, padding='same',\n",
        "                                            kernel_size=3, strides=strides)\n",
        "        conv2 = Conv2D(num_channels, kernel_size=3,\n",
        "                                            padding='same')\n",
        "        conv3 = None\n",
        "        if use_1x1conv:\n",
        "            conv3 = Conv2D(num_channels, kernel_size=1,\n",
        "                                                strides=strides)\n",
        "        bn1 = BatchNormalization()\n",
        "        bn2 = BatchNormalization()\n",
        "        relu1 = Activation(relu)\n",
        "        relu2 = Activation(relu)\n",
        "        Y = relu1((bn1(conv1(X))))\n",
        "        Y = bn2(conv2(Y))\n",
        "        if conv3 is not None:\n",
        "            X = conv3(X)\n",
        "        Y = Add()([Y, X])\n",
        "        return relu2(Y)\n",
        "\n",
        "def make_resnet_block(x, num_channels, num_residuals, first_block=False):\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            x = make_residual_block(x, num_channels, use_1x1conv=True, strides=2)\n",
        "        else:\n",
        "            x = make_residual_block(x, num_channels)\n",
        "    return x\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "main_input = Input(shape=(150, NUM_CEPSTRAL, 1), name='main_input')\n",
        "\n",
        "x = make_resnet_block(main_input, 8, 1, first_block=True)\n",
        "x = make_resnet_block(x, 16, 2, first_block=False)\n",
        "x = make_resnet_block(x, 32, 3, first_block=False)\n",
        "x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "slot_dense = Dense(n_slots*2)(x)\n",
        "slot_reshape = Reshape(target_shape = (2, n_slots))(slot_dense)\n",
        "slot_output = Softmax(name='slot_output')(slot_reshape)\n",
        "\n",
        "intent_output = Dense(n_classes, activation='softmax', name='intent_output', use_bias = False)(x)\n",
        "\n",
        "model = Model(inputs=main_input, outputs=[intent_output, slot_output])\n",
        "\n",
        "optim = Adam(learning_rate=1e-3, weight_decay=1e-6)\n",
        "\n",
        "model.compile(optimizer = optim, loss='categorical_crossentropy', metrics='accuracy')\n",
        "model.summary()\n",
        "\n",
        "if DEBUG:\n",
        "  tf.keras.utils.plot_model(model, to_file='img.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwix84Y4Ih_v"
      },
      "outputs": [],
      "source": [
        "output_path = os.path.join(project_path, datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "os.makedirs(output_path)\n",
        "print(\"Project folder: {}\".format(output_path))\n",
        "\n",
        "model_name = os.path.join(output_path, \"slu_model.h5\")\n",
        "log_dir =  os.path.join(output_path, \"logs\")\n",
        "\n",
        "my_callbacks = [\n",
        "    EarlyStopping(patience=10, restore_best_weights=True, verbose = 1),\n",
        "    ModelCheckpoint(filepath=model_name, save_best_only=True, verbose = 1),\n",
        "    TensorBoard(log_dir=log_dir),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose = 1)]\n",
        "try:\n",
        "    model.fit(training_generator, validation_data = test_generator,\n",
        "              callbacks = my_callbacks, epochs = EPOCH_NUM,\n",
        "              workers = 4, max_queue_size = 10,\n",
        "              use_multiprocessing = False)\n",
        "except KeyboardInterrupt:\n",
        "    raise"
      ],
      "id": "vwix84Y4Ih_v"
    },
    {
      "cell_type": "markdown",
      "id": "340a0256",
      "metadata": {
        "id": "340a0256"
      },
      "source": [
        "### Depthwise convolutions\n",
        "Depthwise convolution blocks allow us to have less multiplication operations in the model by first applying depthwise convolution followed by pointwise convolution. Depthwise Convolution is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D convolution performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb25719",
      "metadata": {
        "id": "dcb25719"
      },
      "outputs": [],
      "source": [
        "def plain_conv_block(inputs, num_filters = 16, alpha = 1, kernel_size = 2, pooling = None, block_id=1, activation = 'relu'):\n",
        "\n",
        "    x = Conv2D(int(num_filters*alpha), kernel_size, padding='same', use_bias = False, name='conv_%d' % block_id)(inputs)\n",
        "    x = BatchNormalization(name='conv_%d_bn' % block_id)(x)\n",
        "    x = Activation(activation, name='conv_%d_act' % block_id)(x)\n",
        "\n",
        "    if pooling:\n",
        "        x = MaxPooling2D(pool_size = pooling, name='conv_%d_pool' % block_id)(x)\n",
        "    return x\n",
        "\n",
        "def dw_conv_block(inputs, num_filters, alpha, depth_multiplier=1, strides=(1, 1), block_id=1, activation = 'relu'):\n",
        "\n",
        "    pointwise_conv_filters = int(num_filters * alpha)\n",
        "\n",
        "    if strides == (1, 1):\n",
        "        x = inputs\n",
        "    else:\n",
        "        x = ZeroPadding2D(((0, 1), (0, 1)),\n",
        "                                 name='conv_pad_%d' % block_id)(inputs)\n",
        "    x = DepthwiseConv2D((2, 2),\n",
        "                               padding='same' if strides == (1, 1) else 'valid',\n",
        "                               depth_multiplier=depth_multiplier,\n",
        "                               #strides=strides,\n",
        "                               use_bias=False,\n",
        "                               name='conv_dw_%d' % block_id)(x)\n",
        "    x = BatchNormalization(name='conv_dw_%d_bn' % block_id)(x)\n",
        "    x = Activation(activation, name='conv_dw_%d_act' % block_id)(x)\n",
        "\n",
        "    x = Conv2D(pointwise_conv_filters, (1, 1),\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      strides=(1, 1),\n",
        "                      name='conv_pw_%d' % block_id)(x)\n",
        "    x = BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)\n",
        "    x = Activation(activation, name='conv_pw_%d_act' % block_id)(x)\n",
        "\n",
        "    if strides > 1:\n",
        "        x = MaxPooling2D(pool_size = 2, name='conv_%d_pool' % block_id)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def _depth(v, divisor=8, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "main_input = Input(shape=(150, NUM_CEPSTRAL, 1), name='main_input')\n",
        "\n",
        "x = plain_conv_block(main_input, num_filters = 16, alpha = 1, kernel_size = 2, pooling = None, block_id=0, activation = 'relu')\n",
        "\n",
        "x = dw_conv_block(x, 16, 2, depth_multiplier=1, strides=1, block_id=1, activation = 'relu')\n",
        "x = dw_conv_block(x, 16, 2, depth_multiplier=1, strides=2, block_id=2, activation = 'relu')\n",
        "x = dw_conv_block(x, 16, 2, depth_multiplier=1, strides=1, block_id=3, activation = 'relu')\n",
        "x = dw_conv_block(x, 32, 2, depth_multiplier=1, strides=2, block_id=4, activation = 'relu')\n",
        "x = dw_conv_block(x, 128, 2, depth_multiplier=1, strides=2, block_id=5, activation = 'relu')\n",
        "\n",
        "x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "slot_dense = Dense(n_slots*2)(x)\n",
        "slot_reshape = Reshape(target_shape = (2, n_slots))(slot_dense)\n",
        "slot_output = Softmax(name='slot_output')(slot_reshape)\n",
        "\n",
        "intent_output = Dense(n_classes, activation='softmax', name='intent_output', use_bias = False)(x)\n",
        "\n",
        "model = Model(inputs=main_input, outputs=[intent_output, slot_output])\n",
        "\n",
        "optim = Adam(learning_rate=1e-3, weight_decay=1e-6)\n",
        "\n",
        "model.compile(optimizer = optim, loss='categorical_crossentropy', metrics='accuracy')\n",
        "model.summary()\n",
        "\n",
        "if DEBUG:\n",
        "  tf.keras.utils.plot_model(model, to_file='img.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ca5a46",
      "metadata": {
        "id": "e6ca5a46"
      },
      "outputs": [],
      "source": [
        "output_path = os.path.join(project_path, datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "os.makedirs(output_path)\n",
        "print(\"Project folder: {}\".format(output_path))\n",
        "\n",
        "model_name = os.path.join(output_path, \"slu_model.h5\")\n",
        "log_dir =  os.path.join(output_path, \"logs\")\n",
        "\n",
        "my_callbacks = [\n",
        "    EarlyStopping(patience=10, restore_best_weights=True, verbose = 1),\n",
        "    ModelCheckpoint(filepath=model_name, save_best_only=True, verbose = 1),\n",
        "    TensorBoard(log_dir=log_dir),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose = 1)]\n",
        "try:\n",
        "    model.fit(training_generator, validation_data = test_generator,\n",
        "              callbacks = my_callbacks, epochs = EPOCH_NUM,\n",
        "              workers = 4, max_queue_size = 10,\n",
        "              use_multiprocessing = False)\n",
        "except KeyboardInterrupt:\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b9ce1e",
      "metadata": {
        "id": "c3b9ce1e"
      },
      "source": [
        "Once training is finished we can check model accuracy on test set and display random sample inference result. Additionally using ```test_models(model_directory=\"checkpoints\")``` we can test all the models in experiment folder and display the names,results and model summaries for models, which slot and intent accuracy is higher than pre-set threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62a9f08c",
      "metadata": {
        "id": "62a9f08c"
      },
      "outputs": [],
      "source": [
        "accuracy_threshold = 0.7\n",
        "\n",
        "def test_models(model_name = None, model_directory = None):\n",
        "\n",
        "    print(\"Testing\")\n",
        "    if model_directory:\n",
        "        model_files_list = []\n",
        "        file_search = lambda ext : glob.glob(model_directory + ext, recursive=True)\n",
        "        for ext in ['/**/*.h5']: model_files_list.extend(file_search(ext))\n",
        "    else:\n",
        "        model_files_list = [model_name]\n",
        "\n",
        "    batch_size = 1\n",
        "\n",
        "    test_generator = DataGenerator([filepaths_test, vectorized_intents_test, vectorized_slots_test],\n",
        "                                         [n_classes, n_slots], batch_size = batch_size, vis = False,\n",
        "                                         shuffle=False, to_fit=True, augment = False)\n",
        "    best_model = None\n",
        "    best_accuracy = 0.0\n",
        "    best_model_file = \"\"\n",
        "\n",
        "    for model_file in model_files_list:\n",
        "\n",
        "        model = load_model(model_file)\n",
        "\n",
        "        intent_correct = 0\n",
        "        slot_correct = 0\n",
        "\n",
        "        for num in range(32):\n",
        "\n",
        "            X, y = test_generator.__getitem__(num)\n",
        "\n",
        "            try:\n",
        "                results = model(X, training=False)\n",
        "            except Exception as e:\n",
        "                print('Error')\n",
        "                break\n",
        "\n",
        "            if ids2intents[np.argmax(y[0])] == ids2intents[np.argmax(results[0])]:\n",
        "                intent_correct += 1\n",
        "\n",
        "            if ids2slots[np.argmax(y[1][0][0])] == ids2slots[np.argmax(results[1][0][0])]:\n",
        "                slot_correct += 1\n",
        "\n",
        "            if ids2slots[np.argmax(y[1][0][1])] == ids2slots[np.argmax(results[1][0][1])]:\n",
        "                slot_correct += 1\n",
        "\n",
        "        accuracy_intent = intent_correct/32\n",
        "        accuracy_slot = slot_correct/64\n",
        "\n",
        "        if accuracy_intent < accuracy_threshold or accuracy_slot < accuracy_threshold and model_directory:\n",
        "            continue\n",
        "\n",
        "        #model.summary()\n",
        "        num = random.randint(0, len(test_generator)-1)\n",
        "\n",
        "        X, y = test_generator.__getitem__(num)\n",
        "\n",
        "        try:\n",
        "            results = model(X, training=False)\n",
        "        except Exception as e:\n",
        "            print('Error')\n",
        "\n",
        "        print(f\"\"\"Model {model_file}\n",
        "\n",
        "        Accuracy Intent {accuracy_intent} %\n",
        "        Accuracy Slot {accuracy_slot} %\n",
        "\n",
        "        Random sample num:{num}\n",
        "\n",
        "        Ground truth\n",
        "        Intent:{ids2intents[np.argmax(y[0])]}\n",
        "        Slot1: {ids2slots[np.argmax(y[1][0][0])]}  Slot2: {ids2slots[np.argmax(y[1][0][1])]}\\n\n",
        "\n",
        "        Prediction\n",
        "        Intent:{ids2intents[np.argmax(results[0])]}\n",
        "        Slot1: {ids2slots[np.argmax(results[1][0][0])]}  Slot2: {ids2slots[np.argmax(results[1][0][1])]}\\n\n",
        "        \"\"\")\n",
        "\n",
        "        if (accuracy_intent + accuracy_slot) / 2 > best_accuracy:\n",
        "            best_model = model\n",
        "            best_model_file = model_file\n",
        "            best_accuracy = (accuracy_intent + accuracy_slot) / 2\n",
        "    return best_model, best_model_file, best_accuracy\n",
        "\n",
        "#model = test_models(model_name = model_name)\n",
        "model, model_name, accuracy = test_models(model_directory = project_path)\n",
        "print(f\"\"\"----------------------------\n",
        "Best model is {model_name} Accuracy {accuracy} %\n",
        "----------------------------\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1578f6",
      "metadata": {
        "id": "5b1578f6"
      },
      "source": [
        "Next cell is simple sanity check on an audio data outside of training/validation/test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4000bbe5",
      "metadata": {
        "id": "4000bbe5"
      },
      "outputs": [],
      "source": [
        "sanity_check_data_prefix = [root_folder, \"data\", \"wavs\", \"wt_test\"]\n",
        "\n",
        "wav_file = os.path.join(*sanity_check_data_prefix, \"change_language_to_chinese_wt.wav\")\n",
        "#wav_file = os.path.join(*sanity_check_data_prefix, \"decrease_volume_wt.wav\")\n",
        "#wav_file = os.path.join(*sanity_check_data_prefix, \"turn_on_the_lights_in_the_kitchen_wt.wav\"\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "sample_rate, audio = wav.read(wav_file)\n",
        "if DEBUG:\n",
        "    print(','.join(str(e) for e in audio.tolist()[:4095]))\n",
        "\n",
        "audio, sample_rate = librosa.load(wav_file, sr=16000, res_type='kaiser_best')\n",
        "audio = librosa.util.fix_length(data = audio, size = 16000*3)\n",
        "features = generate_features(True, audio, SAMPLING_RATE,\n",
        "                  WIN_SIZE_MS, WIN_INCREASE_MS, 32,\n",
        "                  NUM_CEPSTRAL, MIN_FREQ, MAX_FREQ)\n",
        "\n",
        "features = features['features']\n",
        "X = np.expand_dims(features, axis = 0)\n",
        "\n",
        "results = model(X, training=False)\n",
        "print(np.argmax(results[0]), np.argmax(results[1][0][0]), np.argmax(results[1][0][1]))\n",
        "print(f\"\"\"\n",
        "Prediction\n",
        "Intent:{ids2intents[np.argmax(results[0])]}\n",
        "Slot1: {ids2slots[np.argmax(results[1][0][0])]}  Slot2: {ids2slots[np.argmax(results[1][0][1])]}\\n\n",
        "\"\"\")\n",
        "\n",
        "IPython.display.Audio(wav_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d6d7dc",
      "metadata": {
        "id": "73d6d7dc"
      },
      "source": [
        "After you found the model that has satisfying accuracy on a test set, you can convert it to .tflite format. Since we quantize the model to INT8, it is very important to use the audio parameters identical to the ones used in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9373123f",
      "metadata": {
        "id": "9373123f"
      },
      "outputs": [],
      "source": [
        "def representative_dataset():\n",
        "    for i in range(len(test_data)):\n",
        "        wav_file = os.path.join(*prefix, test_data['path'][i])\n",
        "        audio, sample_rate = librosa.load(wav_file, sr=16000, res_type='kaiser_best')\n",
        "        audio = librosa.util.fix_length(data = audio, size = 16000*3)\n",
        "        features = generate_features(False, audio, SAMPLING_RATE,\n",
        "                          WIN_SIZE_MS, WIN_INCREASE_MS, 32,\n",
        "                          NUM_CEPSTRAL, MIN_FREQ, MAX_FREQ)\n",
        "\n",
        "        features = features['features']\n",
        "        X = np.expand_dims(features, axis = -1)\n",
        "        X = np.expand_dims(X, axis = 0)\n",
        "        yield [X.astype(np.float32)]\n",
        "\n",
        "#model = tf.keras.models.load_model(model_name)\n",
        "model.input.set_shape(1 + model.input.shape[1:])\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.experimental_new_converter = True\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "converter.inference_type = tf.int8\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "tflite_filename = os.path.abspath(model_name).split('.')[0] + '.tflite'\n",
        "with open(tflite_filename, 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5065e073",
      "metadata": {
        "id": "5065e073"
      },
      "source": [
        "Next cell allows us to compare the results of INT8 quantized model with the results of FLOAT32 model above to see how much it has affected the model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4da0ab",
      "metadata": {
        "id": "af4da0ab"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path = tflite_filename)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "sanity_check_data_prefix = [*prefix, \"wavs\", \"wt_test\"]\n",
        "\n",
        "wav_file = os.path.join(*sanity_check_data_prefix, \"change_language_to_chinese_wt.wav\")\n",
        "#wav_file = os.path.join(*sanity_check_data_prefix, \"decrease_volume_wt.wav\")\n",
        "#wav_file = os.path.join(*sanity_check_data_prefix, \"turn_on_the_lights_in_the_kitchen_wt.wav\"\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
        "output_scale, output_zero_point = output_details[0][\"quantization\"]\n",
        "\n",
        "audio, sample_rate = librosa.load(wav_file, sr=16000, res_type='kaiser_best')\n",
        "audio = librosa.util.fix_length(data = audio, size = 16000*3)\n",
        "features = generate_features(True, audio, SAMPLING_RATE,\n",
        "                  WIN_SIZE_MS, WIN_INCREASE_MS, 32,\n",
        "                  NUM_CEPSTRAL, MIN_FREQ, MAX_FREQ)\n",
        "\n",
        "features = features['features']\n",
        "\n",
        "X = np.expand_dims(features, axis = -1)\n",
        "X = np.expand_dims(X, axis = 0)\n",
        "\n",
        "input_data = np.asarray(X, dtype=np.float32)\n",
        "\n",
        "input_data_int8 = np.asarray(input_data/input_scale + input_zero_point, dtype=np.int8)\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data_int8)\n",
        "interpreter.invoke()\n",
        "\n",
        "output_data_slot = np.asarray(interpreter.get_tensor(output_details[0]['index']), dtype=np.float32)\n",
        "output_data_intent = np.asarray(interpreter.get_tensor(output_details[1]['index']), dtype=np.float32)\n",
        "\n",
        "intent = (output_data_intent - output_zero_point) * output_scale\n",
        "slot = (output_data_slot - output_zero_point) * output_scale\n",
        "\n",
        "if DEBUG:\n",
        "    print(features)\n",
        "    print(np.argmax(intent[0]), np.argmax(slot[0][0]), np.argmax(slot[0][1]))\n",
        "\n",
        "print(f\"\"\"\n",
        "Prediction\n",
        "Intent:{ids2intents[np.argmax(intent[0])]}\n",
        "Slot1: {ids2slots[np.argmax(slot[0][0])]}  Slot2: {ids2slots[np.argmax(slot[0][1])]}\\n\n",
        "\"\"\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35aa474f",
      "metadata": {
        "id": "35aa474f"
      },
      "source": [
        "Finally we use xxd bash command to create hex dump of the .tflite model weights file to C include file style (option -i)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fc34492",
      "metadata": {
        "id": "8fc34492"
      },
      "outputs": [],
      "source": [
        "tfmicro_filename = tflite_filename.split('.')[0] + '.h'\n",
        "!xxd -i $tflite_filename > $tfmicro_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43ea237d",
      "metadata": {
        "id": "43ea237d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "speech_to_intent_tf_keras_edited.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "a3e04c289b9952ac9b5d38c8f53bb915321c321d3996035803bd3d4f06480882"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}